#!/usr/bin/env python3
"""
Anthropic & Gemini Testing –¥–ª—è v1.2.0
–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ Anthropic –∏ Gemini (OpenAI —É–∂–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω)
"""
import asyncio
import time
import json
from pathlib import Path

from tg_parser.config import settings
from tg_parser.processing import create_processing_pipeline
from tg_parser.storage.sqlite import Database, DatabaseConfig
from tg_parser.storage.sqlite.processed_document_repo import SQLiteProcessedDocumentRepo
from tg_parser.storage.sqlite.processing_failure_repo import SQLiteProcessingFailureRepo
from tg_parser.storage.sqlite.raw_message_repo import SQLiteRawMessageRepo


async def test_provider(
    provider: str,
    model: str,
    message_count: int = 10,
    channel_id: str = "labdiagnostica_logical",
    concurrency: int = 1
):
    """–¢–µ—Å—Ç –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
    print(f"\n{'='*80}")
    print(f"üß™ Testing {provider.upper()} - {model}")
    print(f"{'='*80}\n")
    
    # Database setup
    config = DatabaseConfig(
        ingestion_state_path=settings.ingestion_state_db_path,
        raw_storage_path=settings.raw_storage_db_path,
        processing_storage_path=settings.processing_storage_db_path,
    )
    db = Database(config)
    await db.init()
    
    try:
        raw_session = db.raw_storage_session()
        processing_session = db.processing_storage_session()
        
        try:
            # –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏
            raw_repo = SQLiteRawMessageRepo(raw_session)
            processed_repo = SQLiteProcessedDocumentRepo(processing_session)
            failure_repo = SQLiteProcessingFailureRepo(processing_session)
            
            # Pipeline
            print(f"üîß Creating pipeline for {provider}...")
            pipeline = create_processing_pipeline(
                provider=provider,
                model=model,
                processed_doc_repo=processed_repo,
                failure_repo=failure_repo,
            )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º raw —Å–æ–æ–±—â–µ–Ω–∏—è
            all_raw = await raw_repo.list_by_channel(channel_id)
            if not all_raw:
                print(f"‚ùå No raw messages found for channel {channel_id}")
                return None
            
            # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ N
            raw_messages = all_raw[:message_count]
            print(f"üì¶ Processing {len(raw_messages)} messages (concurrency={concurrency})")
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞
            start = time.time()
            processed_docs = await pipeline.process_batch(
                raw_messages,
                force=True,  # –ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞
                concurrency=concurrency
            )
            elapsed = time.time() - start
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            success_count = len(processed_docs)
            failed_count = len(raw_messages) - success_count
            avg_time = elapsed / len(raw_messages) if raw_messages else 0
            throughput = len(raw_messages) / elapsed if elapsed > 0 else 0
            
            print(f"\nüìä Performance Results:")
            print(f"  ‚úÖ Success: {success_count}/{len(raw_messages)} ({success_count/len(raw_messages)*100:.1f}%)")
            print(f"  ‚ùå Failed: {failed_count}")
            print(f"  ‚è±Ô∏è  Total time: {elapsed:.2f}s")
            print(f"  ‚ö° Throughput: {throughput:.3f} msg/sec")
            print(f"  üìà Avg time per message: {avg_time:.2f}s")
            
            # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
            if processed_docs:
                print(f"\nüîç Quality Analysis:")
                
                quality_metrics = {
                    "has_summary": 0,
                    "has_topics": 0,
                    "has_entities": 0,
                    "correct_language": 0,
                    "avg_summary_len": 0,
                    "avg_topics_count": 0,
                    "avg_entities_count": 0,
                }
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º 3 –ø—Ä–∏–º–µ—Ä–∞
                for i, doc in enumerate(processed_docs[:3], 1):
                    print(f"\n  üìÑ Sample {i}:")
                    print(f"    Summary: {doc.summary[:100]}...")
                    print(f"    Topics: {doc.topics[:4]}")
                    print(f"    Entities: {len(doc.entities)} found")
                    print(f"    Language: {doc.language}")
                
                # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
                for doc in processed_docs:
                    quality_metrics["has_summary"] += 1 if doc.summary and len(doc.summary) > 10 else 0
                    quality_metrics["has_topics"] += 1 if doc.topics and len(doc.topics) > 0 else 0
                    quality_metrics["has_entities"] += 1 if doc.entities and len(doc.entities) > 0 else 0
                    quality_metrics["correct_language"] += 1 if doc.language == "ru" else 0
                    quality_metrics["avg_summary_len"] += len(doc.summary) if doc.summary else 0
                    quality_metrics["avg_topics_count"] += len(doc.topics) if doc.topics else 0
                    quality_metrics["avg_entities_count"] += len(doc.entities) if doc.entities else 0
                
                total = len(processed_docs)
                quality_metrics["avg_summary_len"] /= total
                quality_metrics["avg_topics_count"] /= total
                quality_metrics["avg_entities_count"] /= total
                
                print(f"\n  üìä Aggregate Quality Metrics ({total} docs):")
                print(f"    Summary coverage: {quality_metrics['has_summary']}/{total} ({quality_metrics['has_summary']/total*100:.1f}%)")
                print(f"    Topics coverage: {quality_metrics['has_topics']}/{total} ({quality_metrics['has_topics']/total*100:.1f}%)")
                print(f"    Entities coverage: {quality_metrics['has_entities']}/{total} ({quality_metrics['has_entities']/total*100:.1f}%)")
                print(f"    Language accuracy: {quality_metrics['correct_language']}/{total} ({quality_metrics['correct_language']/total*100:.1f}%)")
                print(f"    Avg summary length: {quality_metrics['avg_summary_len']:.0f} chars")
                print(f"    Avg topics per doc: {quality_metrics['avg_topics_count']:.1f}")
                print(f"    Avg entities per doc: {quality_metrics['avg_entities_count']:.1f}")
                
                return {
                    "provider": provider,
                    "model": model,
                    "success_count": success_count,
                    "failed_count": failed_count,
                    "total_time": elapsed,
                    "avg_time": avg_time,
                    "throughput": throughput,
                    "success_rate": success_count / len(raw_messages) * 100,
                    "quality": quality_metrics,
                }
            else:
                print(f"\n‚ùå No documents were successfully processed")
                return None
            
        finally:
            await raw_session.close()
            await processing_session.close()
            if hasattr(pipeline, "llm_client") and hasattr(pipeline.llm_client, "close"):
                await pipeline.llm_client.close()
    finally:
        await db.close()


async def main():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã Anthropic –∏ Gemini"""
    print("üöÄ TG_parser v1.2.0 ‚Äî Anthropic & Gemini Testing")
    print("=" * 80)
    print("\nüìã Test Configuration:")
    print("  ‚Ä¢ Message count: 10 per provider")
    print("  ‚Ä¢ Concurrency: 1 (baseline)")
    print("  ‚Ä¢ Force reprocess: Yes")
    print("  ‚Ä¢ Channel: labdiagnostica_logical")
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤
    tests = [
        ("anthropic", "claude-sonnet-4-20250514"),  # –ê–∫—Ç—É–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (2025)
        ("gemini", "gemini-2.0-flash-exp"),
    ]
    
    results = []
    
    for provider, model in tests:
        try:
            result = await test_provider(provider, model, message_count=10, concurrency=1)
            if result:
                results.append(result)
        except Exception as e:
            print(f"\n‚ùå Test failed for {provider}: {e}")
            import traceback
            traceback.print_exc()
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏
        if provider != tests[-1][0]:
            print(f"\n‚è∏Ô∏è  Waiting 5 seconds before next provider...")
            await asyncio.sleep(5)
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\n{'='*80}")
    print("üìä SUMMARY")
    print(f"{'='*80}\n")
    
    if results:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã OpenAI –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ç–µ—Å—Ç–∞
        openai_result = None
        try:
            with open("test_results_cloud_providers.json") as f:
                previous_results = json.load(f)
                openai_result = previous_results[0] if previous_results else None
        except:
            pass
        
        # –î–æ–±–∞–≤–ª—è–µ–º OpenAI –∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é –µ—Å–ª–∏ –µ—Å—Ç—å
        all_results = results.copy()
        if openai_result:
            all_results.insert(0, openai_result)
            print("‚ÑπÔ∏è  Including OpenAI results from previous test\n")
        
        # Performance —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
        print("‚ö° Performance Comparison:")
        print(f"{'Provider':<15} {'Model':<30} {'Success':<10} {'Throughput':<15} {'Avg Time':<12}")
        print("-" * 85)
        for r in all_results:
            print(
                f"{r['provider']:<15} "
                f"{r['model']:<30} "
                f"{r['success_rate']:.0f}%{' '*6} "
                f"{r['throughput']:.3f} msg/s{' '*2} "
                f"{r['avg_time']:.2f}s"
            )
        
        # Quality —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
        print(f"\nüìù Quality Comparison:")
        print(f"{'Provider':<15} {'Summary':<12} {'Topics':<12} {'Entities':<12} {'Lang Acc':<12}")
        print("-" * 85)
        for r in all_results:
            q = r['quality']
            total = r['success_count']
            if total > 0:
                print(
                    f"{r['provider']:<15} "
                    f"{q['has_summary']}/{total} ({q['has_summary']/total*100:.0f}%){' '*2} "
                    f"{q['has_topics']}/{total} ({q['has_topics']/total*100:.0f}%){' '*2} "
                    f"{q['has_entities']}/{total} ({q['has_entities']/total*100:.0f}%){' '*2} "
                    f"{q['correct_language']}/{total} ({q['correct_language']/total*100:.0f}%)"
                )
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
        print(f"\nüí° Recommendations:")
        best_perf = max(all_results, key=lambda x: x['throughput'])
        best_quality = max(all_results, key=lambda x: (
            x['quality']['has_summary'] + 
            x['quality']['has_topics'] + 
            x['quality']['has_entities']
        ) if x['success_count'] > 0 else 0)
        
        print(f"  üèÜ Best performance: {best_perf['provider']} ({best_perf['throughput']:.3f} msg/s)")
        print(f"  üèÜ Best quality: {best_quality['provider']}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        output_path = Path("test_results_all_cloud_providers.json")
        with open(output_path, "w") as f:
            json.dump(all_results, f, indent=2, default=str)
        print(f"\nüíæ Full results saved to: {output_path}")
        
    else:
        print("‚ùå No successful tests completed")
        print("\nPossible issues:")
        print("  ‚Ä¢ Check API keys are valid")
        print("  ‚Ä¢ Check account has sufficient balance/credits")
        print("  ‚Ä¢ Check quota limits")


if __name__ == "__main__":
    asyncio.run(main())

