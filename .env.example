# =============================================================================
# TG_parser Environment Configuration
# =============================================================================
# 
# Скопируйте этот файл в .env и заполните своими значениями:
#   cp .env.example .env
#
# Файл .env находится в .gitignore и не попадает в git
#

# =============================================================================
# Telegram API Credentials (обязательно для ingestion)
# =============================================================================
# Получить на https://my.telegram.org/apps

TELEGRAM_API_ID=your_api_id
TELEGRAM_API_HASH=your_api_hash
TELEGRAM_PHONE=+79991234567
TELEGRAM_SESSION_NAME=tg_parser_session


# =============================================================================
# LLM Configuration (v1.2 Multi-LLM Support)
# =============================================================================

# Выбор провайдера (default: openai)
# Опции: openai | anthropic | gemini | ollama
LLM_PROVIDER=openai

# Опционально: переопределение модели
# LLM_MODEL=gpt-4o-mini
# LLM_MODEL=claude-3-5-sonnet-20241022
# LLM_MODEL=gemini-2.0-flash-exp
# LLM_MODEL=llama3.2


# =============================================================================
# OpenAI API Key (для provider=openai)
# =============================================================================
# Получить на https://platform.openai.com/api-keys

OPENAI_API_KEY=sk-...


# =============================================================================
# Anthropic API Key (для provider=anthropic)
# =============================================================================
# Получить на https://console.anthropic.com/

ANTHROPIC_API_KEY=sk-ant-...


# =============================================================================
# Google Gemini API Key (для provider=gemini)
# =============================================================================
# Получить на https://aistudio.google.com/app/apikey

GEMINI_API_KEY=...


# =============================================================================
# Ollama Configuration (для provider=ollama, локальные LLM)
# =============================================================================
# Ollama не требует API key, но нужен запущенный сервер
# 
# Установка:
#   brew install ollama  # macOS
#   # или скачать с https://ollama.com/download
#
# Запуск:
#   ollama serve
#   ollama pull llama3.2
#
# Base URL (default: http://localhost:11434)
# LLM_BASE_URL=http://localhost:11434


# =============================================================================
# LLM Processing Settings
# =============================================================================

# Temperature (0.0 = детерминизм, 1.0 = творчество)
LLM_TEMPERATURE=0.0

# Max tokens в ответе
LLM_MAX_TOKENS=4096

# Retry настройки
PROCESSING_MAX_ATTEMPTS_PER_MESSAGE=3
PROCESSING_RETRY_BACKOFF_BASE=1.0
PROCESSING_RETRY_JITTER_MAX=0.3


# =============================================================================
# Database Paths (можно оставить defaults)
# =============================================================================

# INGESTION_STATE_DB_PATH=ingestion_state.sqlite
# RAW_STORAGE_DB_PATH=raw_storage.sqlite
# PROCESSING_STORAGE_DB_PATH=processing_storage.sqlite


# =============================================================================
# Custom Prompts Directory (опционально)
# =============================================================================
# По умолчанию используется ./prompts/
# Можно переопределить для кастомных промптов

# PROMPTS_DIR=/path/to/custom/prompts


# =============================================================================
# Pipeline Versions (не изменять без необходимости)
# =============================================================================

# PIPELINE_VERSION_PROCESSING=processing:v1.0.0
# PIPELINE_VERSION_TOPICIZATION=topicization:v1.0.0
# EXPORT_VERSION=export:v1.0.0
