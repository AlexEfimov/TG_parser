#!/usr/bin/env python3
"""
Baseline —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ v1.2.0 ‚Äî Multi-LLM providers
–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∫–∞–∂–¥—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º –±–∞—Ç—á–µ —Å–æ–æ–±—â–µ–Ω–∏–π
"""
import asyncio
import time
from pathlib import Path

from tg_parser.config import settings
from tg_parser.processing import create_processing_pipeline
from tg_parser.storage.sqlite import Database, DatabaseConfig
from tg_parser.storage.sqlite.processed_document_repo import SQLiteProcessedDocumentRepo
from tg_parser.storage.sqlite.processing_failure_repo import SQLiteProcessingFailureRepo
from tg_parser.storage.sqlite.raw_message_repo import SQLiteRawMessageRepo


async def test_provider(
    provider: str,
    model: str,
    limit: int = 10,
    channel_id: str = "labdiagnostica_logical"
):
    """–¢–µ—Å—Ç –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –Ω–∞ limit —Å–æ–æ–±—â–µ–Ω–∏—è—Ö"""
    print(f"\n{'='*70}")
    print(f"üß™ Testing {provider.upper()} ({model})")
    print(f"{'='*70}\n")
    
    # Database setup
    config = DatabaseConfig(
        ingestion_state_path=settings.ingestion_state_db_path,
        raw_storage_path=settings.raw_storage_db_path,
        processing_storage_path=settings.processing_storage_db_path,
    )
    db = Database(config)
    await db.init()
    
    try:
        raw_session = db.raw_storage_session()
        processing_session = db.processing_storage_session()
        
        try:
            # –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏
            raw_repo = SQLiteRawMessageRepo(raw_session)
            processed_repo = SQLiteProcessedDocumentRepo(processing_session)
            failure_repo = SQLiteProcessingFailureRepo(processing_session)
            
            # Pipeline
            pipeline = create_processing_pipeline(
                provider=provider,
                model=model,
                processed_doc_repo=processed_repo,
                failure_repo=failure_repo,
            )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º raw —Å–æ–æ–±—â–µ–Ω–∏—è
            all_raw = await raw_repo.list_by_channel(channel_id)
            if not all_raw:
                print(f"‚ùå No raw messages found for channel {channel_id}")
                return None
            
            # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ limit
            raw_messages = all_raw[:limit]
            print(f"üì¶ Loaded {len(raw_messages)} raw messages (–∏–∑ {len(all_raw)} –¥–æ—Å—Ç—É–ø–Ω—ã—Ö)")
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞
            start = time.time()
            processed_docs = await pipeline.process_batch(
                raw_messages,
                force=True,  # –ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å
                concurrency=1  # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è baseline
            )
            elapsed = time.time() - start
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            success_count = len(processed_docs)
            failed_count = len(raw_messages) - success_count
            avg_time = elapsed / len(raw_messages) if raw_messages else 0
            
            print(f"\nüìä Results:")
            print(f"  ‚úÖ Success: {success_count}/{len(raw_messages)} ({success_count/len(raw_messages)*100:.1f}%)")
            print(f"  ‚ùå Failed: {failed_count}")
            print(f"  ‚è±Ô∏è  Total time: {elapsed:.2f}s")
            print(f"  ‚ö° Avg time: {avg_time:.2f}s per message")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            if processed_docs:
                doc = processed_docs[0]
                print(f"\nüîç Quality Check (first document):")
                print(f"  Summary: {doc.summary[:100]}...")
                print(f"  Topics: {doc.topics}")
                print(f"  Language: {doc.language}")
                print(f"  Entities count: {len(doc.entities)}")
            
            return {
                "provider": provider,
                "model": model,
                "success_count": success_count,
                "failed_count": failed_count,
                "total_time": elapsed,
                "avg_time": avg_time,
                "success_rate": success_count / len(raw_messages) * 100,
            }
            
        finally:
            await raw_session.close()
            await processing_session.close()
            if hasattr(pipeline, "llm_client") and hasattr(pipeline.llm_client, "close"):
                await pipeline.llm_client.close()
    finally:
        await db.close()


async def main():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ baseline —Ç–µ—Å—Ç—ã"""
    print("üöÄ TG_parser v1.2.0 ‚Äî Baseline Testing")
    print("=" * 70)
    
    # –¢–µ—Å—Ç—ã
    tests = [
        # ("openai", "gpt-4o-mini"),  # –ó–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ: —Ç—Ä–µ–±—É–µ—Ç API key
        # ("anthropic", "claude-3-5-sonnet-20241022"),  # –¢—Ä–µ–±—É–µ—Ç API key
        ("ollama", "qwen3:8b"),
    ]
    
    results = []
    for provider, model in tests:
        try:
            result = await test_provider(provider, model, limit=10)
            if result:
                results.append(result)
        except Exception as e:
            print(f"\n‚ùå Test failed for {provider}: {e}")
            import traceback
            traceback.print_exc()
    
    # Summary
    print(f"\n{'='*70}")
    print("üìä SUMMARY")
    print(f"{'='*70}\n")
    
    for r in results:
        print(f"{r['provider'].upper()} ({r['model']}):")
        print(f"  Success rate: {r['success_rate']:.1f}%")
        print(f"  Avg time: {r['avg_time']:.2f}s/msg")
        print()


if __name__ == "__main__":
    asyncio.run(main())

