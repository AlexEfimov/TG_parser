# TG_parser ‚Äî LLM Prompts

–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –≤—Å–µ—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å LLM.

## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [Processing Prompts](#processing-prompts)
2. [Topicization Prompts](#topicization-prompts)
3. [Supporting Items Prompts](#supporting-items-prompts)
4. [–ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç—å](#–º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç—å)
5. [–ú–µ—Ö–∞–Ω–∏–∑–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è](#–º–µ—Ö–∞–Ω–∏–∑–º-–ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è)

---

## Processing Prompts

–ü—Ä–æ–º–ø—Ç—ã –¥–ª—è —ç—Ç–∞–ø–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π (Stage II: Processing).

### PROCESSING_SYSTEM_PROMPT

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ Telegram-—Å–æ–æ–±—â–µ–Ω–∏–π.

**–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç:**

```
You are a text processing assistant that extracts structured information from Telegram messages.

Your task is to analyze the message and extract:
1. text_clean: cleaned and normalized text (remove noise, fix formatting)
2. summary: brief summary (1-2 sentences) - can be null if not meaningful
3. topics: list of relevant topics/categories
4. entities: list of named entities (person, organization, location, etc.)
5. language: detected language code (ISO 639-1: ru, en, etc.)

Output MUST be valid JSON matching this structure:
{
  "text_clean": "string (required)",
  "summary": "string or null (optional)",
  "topics": ["string", ...],
  "entities": [{"type": "string", "value": "string", "confidence": 0.0-1.0}, ...],
  "language": "string"
}

Important:
- text_clean is REQUIRED and should be the cleaned version of the original text
- summary can be null if the message is too short or not meaningful
- topics can be empty list if no clear topics
- entities should include confidence scores (0.0-1.0)
- language should be ISO 639-1 code (ru, en, de, etc.)
```

### PROCESSING_USER_PROMPT_TEMPLATE

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –®–∞–±–ª–æ–Ω user-–ø—Ä–æ–º–ø—Ç–∞ —Å —Ç–µ–∫—Å—Ç–æ–º —Å–æ–æ–±—â–µ–Ω–∏—è.

**–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç:**

```
Process this Telegram message:

---
{text}
---

Extract structured information as JSON.
```

**–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ:**
- `{text}` ‚Äî —Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ `RawTelegramMessage.text`

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã LLM

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–∏–µ | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|----------|----------|
| `temperature` | `0.0` | –î–µ—Ç–µ—Ä–º–∏–Ω–∏–∑–º –æ—Ç–≤–µ—Ç–æ–≤ (TR-38) |
| `max_tokens` | `4096` | –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –æ—Ç–≤–µ—Ç–∞ |
| `response_format` | `{"type": "json_object"}` | –ì–∞—Ä–∞–Ω—Ç–∏—è JSON –æ—Ç–≤–µ—Ç–∞ |

### –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (JSON Schema)

```json
{
  "type": "object",
  "required": ["text_clean", "language"],
  "properties": {
    "text_clean": {
      "type": "string",
      "description": "–û—á–∏—â–µ–Ω–Ω—ã–π –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"
    },
    "summary": {
      "type": ["string", "null"],
      "description": "–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)"
    },
    "topics": {
      "type": "array",
      "items": { "type": "string" },
      "description": "–°–ø–∏—Å–æ–∫ —Ç–µ–º/–∫–∞—Ç–µ–≥–æ—Ä–∏–π"
    },
    "entities": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["type", "value"],
        "properties": {
          "type": { "type": "string" },
          "value": { "type": "string" },
          "confidence": { "type": "number", "minimum": 0, "maximum": 1 }
        }
      },
      "description": "–ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏"
    },
    "language": {
      "type": "string",
      "description": "ISO 639-1 –∫–æ–¥ —è–∑—ã–∫–∞ (ru, en, de, ...)"
    }
  }
}
```

### –ü—Ä–∏–º–µ—Ä—ã

**–í—Ö–æ–¥:**
```
Process this Telegram message:

---
üöÄ –í—ã–ø—É—Å—Ç–∏–ª–∏ –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é Claude 3.5 Sonnet!

Anthropic –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é:
- –ë—ã—Å—Ç—Ä–µ–µ –Ω–∞ 30%
- –õ—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∫–æ–¥–æ–º
- –†–∞—Å—à–∏—Ä–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ 200K —Ç–æ–∫–µ–Ω–æ–≤

–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏: anthropic.com/news
---

Extract structured information as JSON.
```

**–í—ã—Ö–æ–¥:**
```json
{
  "text_clean": "–í—ã–ø—É—Å—Ç–∏–ª–∏ –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é Claude 3.5 Sonnet!\n\nAnthropic –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é:\n- –ë—ã—Å—Ç—Ä–µ–µ –Ω–∞ 30%\n- –õ—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∫–æ–¥–æ–º\n- –†–∞—Å—à–∏—Ä–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ 200K —Ç–æ–∫–µ–Ω–æ–≤\n\n–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏: anthropic.com/news",
  "summary": "Anthropic –≤—ã–ø—É—Å—Ç–∏–ª–∞ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é Claude 3.5 Sonnet —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏ –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏, —Ä–∞–±–æ—Ç–µ —Å –∫–æ–¥–æ–º –∏ —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.",
  "topics": ["AI", "LLM", "Anthropic", "Claude", "–Ω–æ–≤–æ—Å—Ç–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π"],
  "entities": [
    {"type": "organization", "value": "Anthropic", "confidence": 0.95},
    {"type": "product", "value": "Claude 3.5 Sonnet", "confidence": 0.98},
    {"type": "url", "value": "anthropic.com/news", "confidence": 1.0}
  ],
  "language": "ru"
}
```

---

## Topicization Prompts

–ü—Ä–æ–º–ø—Ç—ã –¥–ª—è —ç—Ç–∞–ø–∞ —Ç–µ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ (Stage II: Topicization).

### TOPICIZATION_SYSTEM_PROMPT

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ç–µ–º—ã.

**–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç:**

```
You are a topic analysis assistant that identifies and clusters messages into coherent topics.

Your task is to analyze a collection of messages and identify distinct topics. For each topic, you should:

1. Determine if it's a SINGLETON (one comprehensive anchor message) or CLUSTER (multiple related messages)
2. Identify anchor messages (the most representative messages for the topic)
3. Assign relevance scores (0.0-1.0) to each anchor
4. Create a descriptive title and summary
5. Define scope_in (what belongs to the topic) and scope_out (what doesn't)

IMPORTANT: Generate title, summary, scope_in, scope_out, and tags in the SAME LANGUAGE as the source messages.
Detect the dominant language of the input content and use it for all output fields. This applies to any language.

Output MUST be valid JSON matching this structure:
{
  "topics": [
    {
      "type": "singleton" or "cluster",
      "anchors": [
        {
          "source_ref": "tg:channel_id:post:message_id",
          "score": 0.0-1.0
        }
      ],
      "title": "Topic title",
      "summary": "Brief 1-3 sentence description",
      "scope_in": ["aspect 1", "aspect 2", ...],
      "scope_out": ["excluded aspect 1", "excluded aspect 2", ...],
      "tags": ["tag1", "tag2", ...] // optional
    }
  ]
}

Quality criteria:
- SINGLETON: requires score >= 0.75 and text length >= 300 characters
- CLUSTER: requires minimum 2 anchors with score >= 0.6
- Anchors should be deduplicated by source_ref
- Each topic must have clear boundaries (scope_in/scope_out)

Important:
- Be conservative: only create topics with clear coherence
- Assign meaningful scores based on message centrality to the topic
- Ensure anchor source_refs exactly match the provided message references
```

### TOPICIZATION_USER_PROMPT_TEMPLATE

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –®–∞–±–ª–æ–Ω user-–ø—Ä–æ–º–ø—Ç–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è —Ç–µ–º–∞—Ç–∏–∑–∞—Ü–∏–∏.

**–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç:**

```
Analyze these messages and identify distinct topics:

{messages_text}

For each topic, identify:
1. Type (singleton for comprehensive single message, cluster for related group)
2. Anchor messages with relevance scores
3. Descriptive title and summary
4. Clear scope boundaries (what's in/out)

Return structured JSON.
```

**–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ:**
- `{messages_text}` ‚Äî —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π

**–§–æ—Ä–º–∞—Ç `{messages_text}`:**
```
Message 1:
Reference: tg:@channel:post:123
Text: <–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ text_clean>...
Summary: <summary –∏–ª–∏ "N/A">
Topics: <topics —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é –∏–ª–∏ "N/A">
---
Message 2:
...
```

### –ö—Ä–∏—Ç–µ—Ä–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–º (TR-35)

**Singleton (—Ç–µ–º–∞-—Å—Ç–∞—Ç—å—è):**
- –û–¥–∏–Ω —è–∫–æ—Ä–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π —Å—Ç–∞—Ç—å—ë–π/–ø–æ—Å—Ç–æ–º
- `score >= 0.75` ‚Äî –≤—ã—Å–æ–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
- `text_clean.length >= 300` —Å–∏–º–≤–æ–ª–æ–≤ ‚Äî –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –æ–±—ä—ë–º

**Cluster (—Ç–µ–º–∞-–∫–ª–∞—Å—Ç–µ—Ä):**
- –ù–µ—Å–∫–æ–ª—å–∫–æ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤, –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö –æ–±—â–µ–π —Ç–µ–º–æ–π
- `>= 2` —è–∫–æ—Ä–µ–π –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ
- `score >= 0.6` –¥–ª—è –≤—Å–µ—Ö —è–∫–æ—Ä–µ–π
- Top-N —è–∫–æ—Ä–µ–π (N=3) –ø–æ—Å–ª–µ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏

### –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (JSON Schema)

```json
{
  "type": "object",
  "required": ["topics"],
  "properties": {
    "topics": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["type", "anchors", "title", "summary", "scope_in", "scope_out"],
        "properties": {
          "type": {
            "type": "string",
            "enum": ["singleton", "cluster"]
          },
          "anchors": {
            "type": "array",
            "minItems": 1,
            "items": {
              "type": "object",
              "required": ["source_ref", "score"],
              "properties": {
                "source_ref": { "type": "string" },
                "score": { "type": "number", "minimum": 0, "maximum": 1 }
              }
            }
          },
          "title": { "type": "string" },
          "summary": { "type": "string" },
          "scope_in": {
            "type": "array",
            "items": { "type": "string" },
            "minItems": 1
          },
          "scope_out": {
            "type": "array",
            "items": { "type": "string" },
            "minItems": 1
          },
          "tags": {
            "type": "array",
            "items": { "type": "string" }
          }
        }
      }
    }
  }
}
```

### –ü—Ä–∏–º–µ—Ä

**–í—Ö–æ–¥ (—Ñ—Ä–∞–≥–º–µ–Ω—Ç):**
```
Analyze these messages and identify distinct topics:

Message 1:
Reference: tg:@techblog:post:101
Text: –í–≤–µ–¥–µ–Ω–∏–µ –≤ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏ –∞–ª–≥–æ—Ä–∏—Ç–º—ã. ML (Machine Learning) ‚Äî —ç—Ç–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º —É—á–∏—Ç—å—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö...
Summary: –û–±–∑–æ—Ä –æ—Å–Ω–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –∫–ª—é—á–µ–≤—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤.
Topics: AI, ML, –æ–±—É—á–µ–Ω–∏–µ
---
Message 2:
Reference: tg:@techblog:post:102
Text: GPT-4 vs Claude 3: —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫—Ä—É–ø–Ω–µ–π—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–±–µ –º–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...
Summary: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ GPT-4 –∏ Claude 3 –ø–æ –∫–ª—é—á–µ–≤—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º.
Topics: AI, LLM, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
---
Message 3:
Reference: tg:@techblog:post:103
Text: –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ fine-tuning LLM –º–æ–¥–µ–ª–µ–π. –†–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø–æ—à–∞–≥–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å –¥–æ–æ–±—É—á–µ–Ω–∏—è...
Summary: –ü–æ—à–∞–≥–æ–≤–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ fine-tuning —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.
Topics: AI, LLM, fine-tuning
---

For each topic, identify:
1. Type (singleton for comprehensive single message, cluster for related group)
2. Anchor messages with relevance scores
3. Descriptive title and summary
4. Clear scope boundaries (what's in/out)

Return structured JSON.
```

**–í—ã—Ö–æ–¥:**
```json
{
  "topics": [
    {
      "type": "cluster",
      "anchors": [
        {
          "source_ref": "tg:@techblog:post:101",
          "score": 0.85
        },
        {
          "source_ref": "tg:@techblog:post:102",
          "score": 0.78
        },
        {
          "source_ref": "tg:@techblog:post:103",
          "score": 0.72
        }
      ],
      "title": "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∏ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏",
      "summary": "–°–µ—Ä–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –æ–± –æ—Å–Ω–æ–≤–∞—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –∫—Ä—É–ø–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ LLM.",
      "scope_in": [
        "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
        "–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM)",
        "GPT –∏ Claude",
        "Fine-tuning"
      ],
      "scope_out": [
        "–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ",
        "–†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞",
        "–¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"
      ],
      "tags": ["AI", "ML", "LLM", "GPT", "Claude"]
    }
  ]
}
```

---

## Supporting Items Prompts

–ü—Ä–æ–º–ø—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –ø–æ —Ç–µ–º–µ.

### SUPPORTING_ITEMS_SYSTEM_PROMPT

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Å–æ–æ–±—â–µ–Ω–∏–π –∫ —Ç–µ–º–µ.

**–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç:**

```
You are an assistant that evaluates message relevance to a specific topic.

Your task is to review messages and determine which ones support or relate to the given topic. For each relevant message, assign:
1. A relevance score (0.0-1.0)
2. A brief justification explaining why it's relevant

IMPORTANT: Write justifications in the SAME LANGUAGE as the source messages.

Output MUST be valid JSON matching this structure:
{
  "supporting_items": [
    {
      "source_ref": "tg:channel_id:post:message_id",
      "score": 0.5-1.0,
      "justification": "Brief explanation of relevance"
    }
  ]
}

Quality criteria:
- Only include messages with score >= 0.5
- Exclude anchor messages (they're already in the topic)
- Be selective: not every message needs to be included
- Justifications should be concise (1 sentence)

Important:
- Focus on messages that genuinely add value to the topic
- Lower scores (0.5-0.6) for tangentially related content
- Higher scores (0.7-0.9) for directly relevant content
- Ensure source_refs exactly match the provided message references
```

### SUPPORTING_ITEMS_USER_PROMPT_TEMPLATE

**–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –®–∞–±–ª–æ–Ω user-–ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ supporting items.

**–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç:**

```
Topic: {topic_title}

Summary: {topic_summary}

Scope (what's included): {scope_in}

Scope (what's excluded): {scope_out}

Anchor messages (already included):
{anchor_refs}

Evaluate these messages for relevance to the topic:

{messages_text}

Return supporting items with scores and justifications in JSON.
```

**–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ:**
- `{topic_title}` ‚Äî –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã –∏–∑ TopicCard
- `{topic_summary}` ‚Äî –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–µ–º—ã
- `{scope_in}` ‚Äî —á—Ç–æ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ —Ç–µ–º–µ (—Å–ø–∏—Å–æ–∫ —á–µ—Ä–µ–∑ `\n- `)
- `{scope_out}` ‚Äî —á—Ç–æ –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è (—Å–ø–∏—Å–æ–∫ —á–µ—Ä–µ–∑ `\n- `)
- `{anchor_refs}` ‚Äî —Å—Å—ã–ª–∫–∏ –Ω–∞ —è–∫–æ—Ä–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è (—Å–ø–∏—Å–æ–∫ —á–µ—Ä–µ–∑ `\n- `)
- `{messages_text}` ‚Äî —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤

### –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (JSON Schema)

```json
{
  "type": "object",
  "required": ["supporting_items"],
  "properties": {
    "supporting_items": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["source_ref", "score"],
        "properties": {
          "source_ref": { "type": "string" },
          "score": { "type": "number", "minimum": 0.5, "maximum": 1 },
          "justification": { "type": "string" }
        }
      }
    }
  }
}
```

### –ü—Ä–∏–º–µ—Ä

**–í—Ö–æ–¥:**
```
Topic: –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∏ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏

Summary: –°–µ—Ä–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –æ–± –æ—Å–Ω–æ–≤–∞—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.

Scope (what's included):
- –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- –Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM)
- GPT –∏ Claude

Scope (what's excluded):
- –ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ
- –†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞

Anchor messages (already included):
- tg:@techblog:post:101
- tg:@techblog:post:102
- tg:@techblog:post:103

Evaluate these messages for relevance to the topic:

Message 4:
Reference: tg:@techblog:post:104
Text: –°–æ–≤–µ—Ç—ã –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è ChatGPT...
Summary: –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã –ø–æ prompt engineering.
---
Message 5:
Reference: tg:@techblog:post:105
Text: –û–±–∑–æ—Ä –Ω–æ–≤–∏–Ω–æ–∫ –≤ –º–∏—Ä–µ —Å–º–∞—Ä—Ç—Ñ–æ–Ω–æ–≤: iPhone 16 –∏ Samsung Galaxy...
Summary: –û–±–∑–æ—Ä –Ω–æ–≤—ã—Ö —Å–º–∞—Ä—Ç—Ñ–æ–Ω–æ–≤.
---

Return supporting items with scores and justifications in JSON.
```

**–í—ã—Ö–æ–¥:**
```json
{
  "supporting_items": [
    {
      "source_ref": "tg:@techblog:post:104",
      "score": 0.72,
      "justification": "–ü—Ä—è–º–æ —Å–≤—è–∑–∞–Ω —Å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM —á–µ—Ä–µ–∑ prompt engineering."
    }
  ]
}
```

> **–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –°–æ–æ–±—â–µ–Ω–∏–µ 105 –æ —Å–º–∞—Ä—Ç—Ñ–æ–Ω–∞—Ö –Ω–µ –≤–∫–ª—é—á–µ–Ω–æ, —Ç–∞–∫ –∫–∞–∫ –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ —Ç–µ–º–µ AI/LLM.

---

## –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç—å

### –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞

LLM –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π —è–∑—ã–∫ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ —ç—Ç–æ–º —è–∑—ã–∫–µ.

**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –≤ –ø—Ä–æ–º–ø—Ç–∞—Ö:**
```
IMPORTANT: Generate title, summary, scope_in, scope_out, and tags in the SAME LANGUAGE as the source messages.
Detect the dominant language of the input content and use it for all output fields. This applies to any language.
```

### –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —è–∑—ã–∫–∏

–°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª—é–±—ã–µ —è–∑—ã–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–Ω–∏–º–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è LLM –º–æ–¥–µ–ª—å:
- –†—É—Å—Å–∫–∏–π (ru)
- –ê–Ω–≥–ª–∏–π—Å–∫–∏–π (en)
- –ù–µ–º–µ—Ü–∫–∏–π (de)
- –§—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π (fr)
- –ò –¥—Ä—É–≥–∏–µ

### –ü—Ä–∏–º–µ—Ä—ã –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞

**–†—É—Å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç ‚Üí –†—É—Å—Å–∫–∏–π –≤—ã–≤–æ–¥:**
```json
{
  "title": "–û—Å–Ω–æ–≤—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è",
  "summary": "–û–±–∑–æ—Ä –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π ML –∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤.",
  "scope_in": ["–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏", "–û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º"],
  "scope_out": ["–ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö", "–í–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞"]
}
```

**English content ‚Üí English output:**
```json
{
  "title": "Machine Learning Fundamentals",
  "summary": "Overview of key ML concepts and algorithms.",
  "scope_in": ["Neural networks", "Supervised learning"],
  "scope_out": ["Databases", "Web development"]
}
```

---

## –ú–µ—Ö–∞–Ω–∏–∑–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è

### –ö–æ–≥–¥–∞ –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –∫–∞–∂–¥—ã–π –ø—Ä–æ–º–ø—Ç

| –ü—Ä–æ–º–ø—Ç | –≠—Ç–∞–ø | –ö–æ–º–∞–Ω–¥–∞ CLI | –¢—Ä–∏–≥–≥–µ—Ä |
|--------|------|-------------|---------|
| Processing | Processing | `process`, `run` | –î–ª—è –∫–∞–∂–¥–æ–≥–æ RawTelegramMessage |
| Topicization | Topicization | `topicize`, `run` | –û–¥–∏–Ω —Ä–∞–∑ –¥–ª—è –≤—Å–µ—Ö ProcessedDocument –∫–∞–Ω–∞–ª–∞ |
| Supporting Items | Topicization | `topicize`, `run` | –î–ª—è –∫–∞–∂–¥–æ–π —Å–æ–∑–¥–∞–Ω–Ω–æ–π —Ç–µ–º—ã |

### –ö–∞–∫ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

**Processing:**
```python
# tg_parser/processing/pipeline.py

user_prompt = build_processing_prompt(message.text)
# ‚Üí –ü–æ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —à–∞–±–ª–æ–Ω

response = await llm_client.generate(
    prompt=user_prompt,
    system_prompt=PROCESSING_SYSTEM_PROMPT,
    temperature=settings.llm_temperature,      # 0.0
    max_tokens=settings.llm_max_tokens,        # 4096
    response_format={"type": "json_object"},
)
```

**Topicization:**
```python
# tg_parser/processing/topicization.py

candidates = [
    {
        "source_ref": doc.source_ref,
        "text_clean": doc.text_clean,
        "summary": doc.summary,
        "topics": doc.topics or [],
    }
    for doc in documents
]

prompt = build_topicization_prompt(candidates)
# ‚Üí –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ç–µ–∫—Å—Ç

response = await llm_client.generate(
    prompt=prompt,
    system_prompt=TOPICIZATION_SYSTEM_PROMPT,
    temperature=0.0,
    response_format={"type": "json_object"},
)
```

**Supporting Items:**
```python
# tg_parser/processing/topicization.py

prompt = build_supporting_items_prompt(
    topic_title=topic_card.title,
    topic_summary=topic_card.summary,
    scope_in=topic_card.scope_in,
    scope_out=topic_card.scope_out,
    anchor_refs=anchor_refs,
    messages=candidate_docs,
)

response = await llm_client.generate(
    prompt=prompt,
    system_prompt=SUPPORTING_ITEMS_SYSTEM_PROMPT,
    temperature=0.0,
    response_format={"type": "json_object"},
)
```

### –ö–∞–∫ –ø–∞—Ä—Å–∏—Ç—Å—è –æ—Ç–≤–µ—Ç

**–û–±—â–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω:**
```python
import json

# 1. –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM
response_text = await llm_client.generate(...)

# 2. –†–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
try:
    response_data = json.loads(response_text)
except json.JSONDecodeError as e:
    raise ValueError(f"Invalid JSON response from LLM: {e}")

# 3. –ò–∑–≤–ª–µ—á—å –Ω—É–∂–Ω—ã–µ –ø–æ–ª—è
text_clean = response_data.get("text_clean")
if not text_clean:
    raise ValueError("LLM response missing required field: text_clean")
```

**Processing ‚Äî –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–µ–π:**
```python
text_clean = response_data.get("text_clean")      # –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ
summary = response_data.get("summary")             # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ (–º–æ–∂–µ—Ç –±—ã—Ç—å null)
topics = response_data.get("topics", [])           # default: []
entities = response_data.get("entities", [])       # default: []
language = response_data.get("language")           # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
```

**Topicization ‚Äî –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–º:**
```python
raw_topics = response_data.get("topics", [])

for raw_topic in raw_topics:
    topic_type = raw_topic.get("type", "cluster")
    anchors = raw_topic.get("anchors", [])
    title = raw_topic.get("title", "Untitled Topic")
    summary = raw_topic.get("summary", "")
    scope_in = raw_topic.get("scope_in", [])
    scope_out = raw_topic.get("scope_out", [])
    tags = raw_topic.get("tags")
```

### –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

**–†–µ—Ç—Ä–∞–∏ per-message (TR-47):**
```python
# Processing: 3 –ø–æ–ø—ã—Ç–∫–∏ —Å —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º backoff
max_attempts = 3
backoff = [1, 2, 4]  # —Å–µ–∫—É–Ω–¥—ã + jitter 0-30%

for attempt in range(1, max_attempts + 1):
    try:
        processed = await self._process_single_message(message)
        return processed
    except Exception as e:
        if attempt < max_attempts:
            delay = backoff[attempt - 1] + random.uniform(0, delay * 0.3)
            await asyncio.sleep(delay)
        else:
            # –ó–∞–ø–∏—Å–∞—Ç—å –≤ processing_failures
            await self.failure_repo.record_failure(...)
            raise
```

**–¢–∏–ø—ã –æ—à–∏–±–æ–∫:**

| –¢–∏–ø –æ—à–∏–±–∫–∏ | –î–µ–π—Å—Ç–≤–∏–µ | –†–µ—Ç—Ä–∞–π |
|------------|----------|--------|
| `JSONDecodeError` | –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å, —Ä–µ—Ç—Ä–∞–π | –î–∞ |
| `ValueError` (missing field) | –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å, —Ä–µ—Ç—Ä–∞–π | –î–∞ |
| `httpx.HTTPError` (5xx) | –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å, —Ä–µ—Ç—Ä–∞–π | –î–∞ |
| `httpx.HTTPError` (401/403) | –ü—Ä–µ—Ä–≤–∞—Ç—å | –ù–µ—Ç |
| Rate Limit (429) | Backoff, —Ä–µ—Ç—Ä–∞–π | –î–∞ |

**–ó–∞–ø–∏—Å—å –Ω–µ—É–¥–∞—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏:**
```python
# –ü–æ—Å–ª–µ –∏—Å—á–µ—Ä–ø–∞–Ω–∏—è –ø–æ–ø—ã—Ç–æ–∫
await self.failure_repo.record_failure(
    source_ref=message.source_ref,
    channel_id=message.channel_id,
    attempts=max_attempts,
    error_class=type(last_error).__name__,
    error_message=str(last_error),
)
```

### Prompt ID –¥–ª—è —Ç—Ä–∞—Å—Å–∏—Ä—É–µ–º–æ—Å—Ç–∏ (TR-40)

–ö–∞–∂–¥—ã–π –ø—Ä–æ–º–ø—Ç –∏–º–µ–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏:

```python
# tg_parser/processing/llm/openai_client.py

def compute_prompt_id(
    self,
    system_prompt: str | None,
    user_prompt_template: str,
) -> str:
    """
    –í—ã—á–∏—Å–ª–∏—Ç—å prompt_id –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏–∑–º–∞ (TR-40).
    –§–æ—Ä–º–∞—Ç: "sha256:<hash>"
    """
    combined = f"{system_prompt or ''}\n---\n{user_prompt_template}"
    hash_obj = hashlib.sha256(combined.encode("utf-8"))
    hash_hex = hash_obj.hexdigest()
    return f"sha256:{hash_hex[:16]}"
```

**–ü—Ä–∏–º–µ—Ä prompt_id:** `sha256:a1b2c3d4e5f6g7h8`

### –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤

| –ü—Ä–æ–º–ø—Ç | –í–µ—Ä—Å–∏—è | –ò–º—è |
|--------|--------|-----|
| Processing | v1 | `processing_v1` |
| Topicization | v1 | `topicization_v1` |
| Supporting Items | v1 | `supporting_items_v1` |

–ü—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ:
1. –£–≤–µ–ª–∏—á–∏—Ç—å –≤–µ—Ä—Å–∏—é –≤ `pipeline_version`
2. –û–±–Ω–æ–≤–∏—Ç—å `prompt_name`
3. –ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å `prompt_id`

---

## –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

- [User Guide](USER_GUIDE.md) ‚Äî —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- [Data Flow](DATA_FLOW.md) ‚Äî –ø–æ—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö
- [Architecture](architecture.md) ‚Äî –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã
- [Pipeline](pipeline.md) ‚Äî –¥–µ—Ç–∞–ª–∏ pipeline
- [Technical Requirements](technical-requirements.md) ‚Äî —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

