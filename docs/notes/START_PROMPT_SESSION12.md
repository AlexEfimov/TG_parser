# Session 12: v1.2 Developer Agent ‚Äî "Multi-LLM & Performance"

## –†–æ–ª—å

–ü—Ä–∏–≤–µ—Ç! –¢—ã Developer Agent –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Ä—Å–∏–∏ **v1.2.0** –ø—Ä–æ–µ–∫—Ç–∞ TG_parser. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å **Multi-LLM Support** –∏ —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã.

---

## üìã –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞

**TG_parser** ‚Äî production-ready —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–±–æ—Ä–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ Telegram-–∫–∞–Ω–∞–ª–æ–≤, –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–µ–∑ LLM –∏ —ç–∫—Å–ø–æ—Ä—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

### –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (v1.1)
- ‚úÖ **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å**: 99.76% —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –Ω–∞ 846 —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏—è—Ö
- ‚úÖ **–¢–µ—Å—Ç—ã**: 103 —Ç–µ—Å—Ç–∞, 100% –ø—Ä–æ—Ö–æ–¥—è—Ç
- ‚úÖ **Configurable Prompts**: YAML —Ñ–∞–π–ª—ã –≤ `prompts/`
- ‚úÖ **PromptLoader**: –≥–æ—Ç–æ–≤ –∫ Multi-LLM
- ‚úÖ **TODOs**: 0 –≤ –∫–æ–¥–µ
- ‚ö†Ô∏è **–¢–æ–ª—å–∫–æ OpenAI**: –Ω–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∫–∏ Claude/Gemini/Ollama
- ‚ö†Ô∏è **–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞**: 30 –º–∏–Ω –Ω–∞ 846 —Å–æ–æ–±—â–µ–Ω–∏–π

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
```
tg_parser/
‚îú‚îÄ‚îÄ cli/           # Typer CLI (7 –∫–æ–º–∞–Ω–¥)
‚îú‚îÄ‚îÄ config/        # Pydantic-settings
‚îú‚îÄ‚îÄ domain/        # –î–æ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (Pydantic v2)
‚îú‚îÄ‚îÄ ingestion/     # Telethon client
‚îú‚îÄ‚îÄ processing/    # LLM processing ‚Üê –û–°–ù–û–í–ù–û–ô –§–û–ö–£–°
‚îÇ   ‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_client.py   # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anthropic_client.py  # ‚≠ê NEW
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gemini_client.py     # ‚≠ê NEW
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama_client.py     # ‚≠ê NEW
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ factory.py           # ‚≠ê NEW
‚îÇ   ‚îú‚îÄ‚îÄ prompt_loader.py     # v1.1 ‚Äî –≥–æ—Ç–æ–≤
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py
‚îÇ   ‚îú‚îÄ‚îÄ prompts.py
‚îÇ   ‚îú‚îÄ‚îÄ topicization_prompts.py
‚îÇ   ‚îî‚îÄ‚îÄ topicization.py
‚îú‚îÄ‚îÄ storage/       # SQLite —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏
‚îî‚îÄ‚îÄ export/        # –≠–∫—Å–ø–æ—Ä—Ç KB entries + topics
```

---

## üìö –î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

### üî¥ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ø—Ä–æ—á–∏—Ç–∞—Ç—å –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º —Ä–∞–±–æ—Ç—ã

| –î–æ–∫—É–º–µ–Ω—Ç | –û–ø–∏—Å–∞–Ω–∏–µ | –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç |
|----------|----------|-----------|
| `DEVELOPMENT_ROADMAP.md` | **–ü–ª–∞–Ω v1.2** ‚Äî –¥–µ—Ç–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏ –∫—Ä–∏—Ç–µ—Ä–∏–∏ | ‚≠ê‚≠ê‚≠ê |
| `docs/notes/SESSION_HANDOFF_v1.1.md` | –ß—Ç–æ —Å–¥–µ–ª–∞–Ω–æ –≤ v1.1 | ‚≠ê‚≠ê‚≠ê |
| `tg_parser/processing/llm/openai_client.py` | –¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è LLM ‚Äî –æ–±—Ä–∞–∑–µ—Ü –¥–ª—è –Ω–æ–≤—ã—Ö | ‚≠ê‚≠ê‚≠ê |
| `tg_parser/processing/ports.py` | –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å LLMClient | ‚≠ê‚≠ê‚≠ê |
| `tg_parser/processing/prompt_loader.py` | PromptLoader ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤ | ‚≠ê‚≠ê |

### üü° –î–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã

| –î–æ–∫—É–º–µ–Ω—Ç | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|----------|
| `README.md` | –û–±—â–∏–π –æ–±–∑–æ—Ä, CLI –∫–æ–º–∞–Ω–¥—ã |
| `CHANGELOG.md` | –ò—Å—Ç–æ—Ä–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π |
| `docs/architecture.md` | –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã |
| `docs/LLM_PROMPTS.md` | –¢–µ–∫—É—â–∏–µ –ø—Ä–æ–º–ø—Ç—ã |
| `prompts/README.md` | –§–æ—Ä–º–∞—Ç YAML –ø—Ä–æ–º–ø—Ç–æ–≤ |

### üü¢ –î–ª—è —Å–ø—Ä–∞–≤–∫–∏ –ø–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

| –î–æ–∫—É–º–µ–Ω—Ç | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|----------|
| `tg_parser/processing/pipeline.py` | Processing pipeline |
| `tg_parser/config/settings.py` | –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ‚Äî –¥–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ API keys |
| `tests/test_processing_pipeline.py` | –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ—Å—Ç—ã |

---

## üì§ –î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å–ª–µ–¥—É—é—â–∏–º –∞–≥–µ–Ω—Ç–∞–º

### –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è

| –î–æ–∫—É–º–µ–Ω—Ç | –ß—Ç–æ –æ–±–Ω–æ–≤–∏—Ç—å |
|----------|--------------|
| `docs/notes/SESSION_HANDOFF_v1.2.md` | ‚≠ê **–°–û–ó–î–ê–¢–¨** ‚Äî handoff –¥–ª—è v2.0 –∞–≥–µ–Ω—Ç–∞ |
| `DEVELOPMENT_ROADMAP.md` | –û—Ç–º–µ—Ç–∏—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ v1.2 |
| `CHANGELOG.md` | –î–æ–±–∞–≤–∏—Ç—å v1.2 –∏–∑–º–µ–Ω–µ–Ω–∏—è |
| `docs/notes/current-state.md` | –û–±–Ω–æ–≤–∏—Ç—å —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ |

---

## üéØ –ó–∞–¥–∞—á–∏ v1.2.0 (–ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω—ã)

### –ù–µ–¥–µ–ª—è 1-2 ‚Äî High Priority

#### –ó–∞–¥–∞—á–∞ 1: ‚≠ê Multi-LLM Support (Chat Completions API)
**–í—Ä–µ–º—è**: 12 —á–∞—Å–æ–≤  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç**: HIGHEST

**–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã:**
```
tg_parser/processing/llm/
‚îú‚îÄ‚îÄ anthropic_client.py  # ‚≠ê NEW
‚îú‚îÄ‚îÄ gemini_client.py     # ‚≠ê NEW
‚îú‚îÄ‚îÄ ollama_client.py     # ‚≠ê NEW
‚îî‚îÄ‚îÄ factory.py           # ‚≠ê NEW
```

**AnthropicClient (anthropic_client.py):**
```python
"""
Anthropic Claude LLM –∫–ª–∏–µ–Ω—Ç.
–†–µ–∞–ª–∏–∑—É–µ—Ç LLMClient –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è Claude models.
"""

import json
import logging
from typing import Any

import httpx

from tg_parser.processing.ports import LLMClient

logger = logging.getLogger(__name__)


class AnthropicClient(LLMClient):
    """
    Anthropic Claude –∫–ª–∏–µ–Ω—Ç —á–µ—Ä–µ–∑ Messages API.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏:
    - claude-3-5-sonnet-20241022
    - claude-3-5-haiku-20241022
    - claude-3-opus-20240229
    """

    BASE_URL = "https://api.anthropic.com/v1/messages"
    API_VERSION = "2023-06-01"

    def __init__(
        self,
        api_key: str,
        model: str = "claude-3-5-sonnet-20241022",
        max_tokens: int = 4096,
    ):
        self.api_key = api_key
        self.model = model
        self.max_tokens = max_tokens
        self._client = httpx.AsyncClient(timeout=120.0)

    async def generate(
        self,
        prompt: str,
        system_prompt: str | None = None,
        temperature: float = 0.0,
        max_tokens: int | None = None,
        response_format: dict | None = None,
        **kwargs,
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ Anthropic Messages API.
        
        Args:
            prompt: User prompt
            system_prompt: System prompt
            temperature: Temperature (0-1)
            max_tokens: Max tokens –≤ –æ—Ç–≤–µ—Ç–µ
            response_format: {"type": "json_object"} –¥–ª—è JSON mode
            
        Returns:
            –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
        """
        headers = {
            "x-api-key": self.api_key,
            "anthropic-version": self.API_VERSION,
            "content-type": "application/json",
        }

        messages = [{"role": "user", "content": prompt}]

        # Claude –∏—Å–ø–æ–ª—å–∑—É–µ—Ç system prompt –æ—Ç–¥–µ–ª—å–Ω–æ
        payload = {
            "model": self.model,
            "max_tokens": max_tokens or self.max_tokens,
            "temperature": temperature,
            "messages": messages,
        }

        if system_prompt:
            payload["system"] = system_prompt

        # JSON mode hint –≤ prompt (Claude –Ω–µ –∏–º–µ–µ—Ç response_format)
        if response_format and response_format.get("type") == "json_object":
            # –î–æ–±–∞–≤–ª—è–µ–º hint –æ JSON –≤ –∫–æ–Ω–µ—Ü prompt
            if "JSON" not in prompt:
                messages[0]["content"] = prompt + "\n\nRespond with valid JSON only."

        try:
            response = await self._client.post(
                self.BASE_URL,
                headers=headers,
                json=payload,
            )
            response.raise_for_status()

            data = response.json()
            content = data["content"][0]["text"]

            logger.debug(
                "Anthropic response received",
                extra={
                    "model": self.model,
                    "input_tokens": data.get("usage", {}).get("input_tokens"),
                    "output_tokens": data.get("usage", {}).get("output_tokens"),
                },
            )

            return content

        except httpx.HTTPStatusError as e:
            logger.error(f"Anthropic API error: {e.response.status_code} - {e.response.text}")
            raise
        except Exception as e:
            logger.error(f"Anthropic request failed: {e}")
            raise

    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç—å HTTP –∫–ª–∏–µ–Ω—Ç."""
        await self._client.aclose()

    def compute_prompt_id(self, system_prompt: str, user_prompt: str) -> str:
        """Compute stable hash of prompts for caching/tracking."""
        import hashlib
        combined = f"{system_prompt}||{user_prompt}"
        return hashlib.sha256(combined.encode()).hexdigest()[:16]
```

**Factory (factory.py):**
```python
"""
LLM Client Factory.
–°–æ–∑–¥–∞—ë—Ç LLM –∫–ª–∏–µ–Ω—Ç –ø–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—É.
"""

from typing import Any

from tg_parser.processing.ports import LLMClient


def create_llm_client(
    provider: str,
    api_key: str | None = None,
    model: str | None = None,
    base_url: str | None = None,
    **kwargs: Any,
) -> LLMClient:
    """
    –°–æ–∑–¥–∞—Ç—å LLM –∫–ª–∏–µ–Ω—Ç –ø–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—É.
    
    Args:
        provider: "openai" | "anthropic" | "gemini" | "ollama"
        api_key: API –∫–ª—é—á –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        model: –ú–æ–¥–µ–ª—å (default –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞)
        base_url: Custom base URL (–¥–ª—è Ollama –∏–ª–∏ –ø—Ä–æ–∫—Å–∏)
        
    Returns:
        LLMClient instance
        
    Raises:
        ValueError: –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
    """
    provider = provider.lower()
    
    if provider == "openai":
        from .openai_client import OpenAIClient
        return OpenAIClient(
            api_key=api_key,
            model=model or "gpt-4o-mini",
            base_url=base_url,
        )
    
    elif provider == "anthropic":
        from .anthropic_client import AnthropicClient
        if not api_key:
            raise ValueError("Anthropic API key required")
        return AnthropicClient(
            api_key=api_key,
            model=model or "claude-3-5-sonnet-20241022",
        )
    
    elif provider == "gemini":
        from .gemini_client import GeminiClient
        if not api_key:
            raise ValueError("Gemini API key required")
        return GeminiClient(
            api_key=api_key,
            model=model or "gemini-2.0-flash",
        )
    
    elif provider == "ollama":
        from .ollama_client import OllamaClient
        return OllamaClient(
            base_url=base_url or "http://localhost:11434",
            model=model or "llama3.2",
        )
    
    else:
        raise ValueError(
            f"Unknown LLM provider: {provider}. "
            f"Supported: openai, anthropic, gemini, ollama"
        )
```

**–û–±–Ω–æ–≤–∏—Ç—å settings.py:**
```python
# ==========================================================================
# LLM –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (v1.2 Multi-LLM)
# ==========================================================================

llm_provider: str = "openai"  # openai | anthropic | gemini | ollama
llm_model: str | None = None  # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
llm_base_url: str | None = None  # –î–ª—è OpenAI-compatible –ø—Ä–æ–∫—Å–∏ –∏–ª–∏ Ollama

# API keys (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ ENV)
openai_api_key: str | None = None
anthropic_api_key: str | None = None
gemini_api_key: str | None = None
```

**CLI —Ñ–ª–∞–≥:**
```python
@app.command()
def process(
    channel: str = typer.Option(...),
    force: bool = typer.Option(False),
    retry_failed: bool = typer.Option(False),
    provider: str = typer.Option(None, "--provider", help="LLM provider override"),
    model: str = typer.Option(None, "--model", help="Model override"),
):
    """Process raw messages through LLM."""
    ...
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:**
- [ ] AnthropicClient —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω
- [ ] GeminiClient —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω  
- [ ] OllamaClient —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω
- [ ] Factory —Å–æ–∑–¥–∞—ë—Ç –∫–ª–∏–µ–Ω—Ç –ø–æ `LLM_PROVIDER`
- [ ] CLI `--provider` –∏ `--model` —Ä–∞–±–æ—Ç–∞—é—Ç
- [ ] –¢–µ—Å—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞
- [ ] –ü—Ä–æ–º–ø—Ç—ã —Ä–∞–±–æ—Ç–∞—é—Ç —Å–æ –≤—Å–µ–º–∏ –º–æ–¥–µ–ª—è–º–∏

---

#### –ó–∞–¥–∞—á–∞ 2: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
**–í—Ä–µ–º—è**: 8 —á–∞—Å–æ–≤
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç**: HIGH

**–§–∞–π–ª**: `tg_parser/processing/pipeline.py`

```python
import asyncio
from typing import Any

async def process_batch_parallel(
    self,
    messages: list[RawTelegramMessage],
    force: bool = False,
    concurrency: int = 5,
) -> list[ProcessedDocument]:
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ —Å–æ–æ–±—â–µ–Ω–∏–π.
    
    TR-47: –æ—à–∏–±–∫–∞ –Ω–∞ –æ–¥–Ω–æ–º —Å–æ–æ–±—â–µ–Ω–∏–∏ –Ω–µ –¥–æ–ª–∂–Ω–∞ —Ä–æ–Ω—è—Ç—å –≤–µ—Å—å –±–∞—Ç—á.
    
    Args:
        messages: –°–ø–∏—Å–æ–∫ RawTelegramMessage
        force: –ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–∂–µ –µ—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å processed
        concurrency: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        
    Returns:
        –°–ø–∏—Å–æ–∫ ProcessedDocument
    """
    semaphore = asyncio.Semaphore(concurrency)
    results: list[ProcessedDocument | None] = []
    
    async def process_with_semaphore(message: RawTelegramMessage) -> ProcessedDocument | None:
        async with semaphore:
            try:
                return await self.process_message(message, force=force)
            except Exception as e:
                logger.error(f"Failed to process {message.source_ref}: {e}")
                return None
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    tasks = [process_with_semaphore(msg) for msg in messages]
    results = await asyncio.gather(*tasks)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º None (failed)
    successful = [r for r in results if r is not None]
    
    logger.info(
        f"Parallel batch complete: {len(successful)}/{len(messages)} successful"
    )
    
    return successful
```

**CLI —Ñ–ª–∞–≥:**
```python
@app.command()
def process(
    ...
    concurrency: int = typer.Option(5, "--concurrency", "-c", help="Parallel requests"),
):
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:**
- [ ] `--concurrency N` —Ñ–ª–∞–≥ —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] Rate limiting —Å–æ–±–ª—é–¥–∞–µ—Ç—Å—è
- [ ] –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ 3-5x –±—ã—Å—Ç—Ä–µ–µ
- [ ] –¢–µ—Å—Ç—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏

---

#### –ó–∞–¥–∞—á–∞ 3: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è PromptLoader –≤ pipeline
**–í—Ä–µ–º—è**: 4 —á–∞—Å–∞
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç**: HIGH

**–§–∞–π–ª**: `tg_parser/processing/pipeline.py`

```python
from tg_parser.processing.prompt_loader import PromptLoader, get_prompt_loader

class ProcessingPipelineImpl(ProcessingPipeline):
    def __init__(
        self,
        llm_client: LLMClient,
        processed_doc_repo: ProcessedDocumentRepo,
        failure_repo: ProcessingFailureRepo | None = None,
        prompt_loader: PromptLoader | None = None,  # NEW
        ...
    ):
        ...
        self.prompt_loader = prompt_loader or get_prompt_loader()
    
    async def _process_single_message(self, message: RawTelegramMessage) -> ProcessedDocument:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–º–ø—Ç—ã –∏–∑ YAML
        system_prompt = self.prompt_loader.get_system_prompt("processing")
        user_template = self.prompt_loader.get_user_template("processing")
        model_settings = self.prompt_loader.get_model_settings("processing")
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º user prompt
        user_prompt = user_template.format(text=message.text)
        
        # –í—ã–∑—ã–≤–∞–µ–º LLM
        response_text = await self.llm_client.generate(
            prompt=user_prompt,
            system_prompt=system_prompt,
            **model_settings,
        )
        ...
```

**CLI —Ñ–ª–∞–≥:**
```python
@app.callback()
def main(
    prompts_dir: Path = typer.Option(
        None, "--prompts-dir", help="Custom prompts directory"
    ),
):
    if prompts_dir:
        from tg_parser.processing.prompt_loader import PromptLoader, set_prompt_loader
        set_prompt_loader(PromptLoader(prompts_dir))
```

---

#### –ó–∞–¥–∞—á–∞ 4: Dockerfile
**–í—Ä–µ–º—è**: 4 —á–∞—Å–∞

**–§–∞–π–ª**: `Dockerfile`

```dockerfile
# Multi-stage build for production
FROM python:3.12-slim AS builder

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

# Production stage
FROM python:3.12-slim

WORKDIR /app

# Copy dependencies from builder
COPY --from=builder /root/.local /root/.local
ENV PATH=/root/.local/bin:$PATH

# Copy application
COPY tg_parser/ ./tg_parser/
COPY prompts/ ./prompts/
COPY pyproject.toml .

# Install package
RUN pip install --no-cache-dir -e .

# Default command
ENTRYPOINT ["python", "-m", "tg_parser.cli"]
CMD ["--help"]
```

**–§–∞–π–ª**: `docker-compose.yml`

```yaml
version: '3.8'

services:
  tg_parser:
    build: .
    volumes:
      - ./data:/app/data
      - ./.env:/app/.env:ro
      - ./prompts:/app/prompts:ro
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
    command: ["run", "--source", "${SOURCE_ID}", "--out", "/app/data/output"]
```

---

#### –ó–∞–¥–∞—á–∞ 5: GitHub Actions CI
**–í—Ä–µ–º—è**: 4 —á–∞—Å–∞

**–§–∞–π–ª**: `.github/workflows/ci.yml`

```yaml
name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install -e ".[dev]"
          pip install pytest-cov
      
      - name: Run linting
        run: ruff check .
      
      - name: Run tests
        run: pytest --tb=short -v --cov=tg_parser
      
      - name: Upload coverage
        uses: codecov/codecov-action@v4
        with:
          fail_ci_if_error: false

  docker:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Build Docker image
        run: docker build -t tg_parser:test .
      
      - name: Test Docker image
        run: docker run tg_parser:test --help
```

---

### –ù–µ–¥–µ–ª—è 3-4 ‚Äî Medium Priority

#### –ó–∞–¥–∞—á–∞ 6: Progress bars –∏ —Ü–≤–µ—Ç–∞ –≤ CLI
**–í—Ä–µ–º—è**: 4 —á–∞—Å–∞

```bash
pip install rich
```

```python
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.console import Console

console = Console()

with Progress(
    SpinnerColumn(),
    TextColumn("[progress.description]{task.description}"),
    transient=True,
) as progress:
    task = progress.add_task("Processing messages...", total=len(messages))
    for msg in messages:
        await pipeline.process_message(msg)
        progress.advance(task)
```

#### –ó–∞–¥–∞—á–∞ 7: Dry-run mode
**–í—Ä–µ–º—è**: 3 —á–∞—Å–∞

```python
@app.command()
def process(
    ...
    dry_run: bool = typer.Option(False, "--dry-run", help="Preview without changes"),
):
    if dry_run:
        console.print("[yellow]DRY RUN MODE[/yellow]")
        console.print(f"Would process {len(messages)} messages")
        return
```

---

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```bash
# –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å –æ–∫—Ä—É–∂–µ–Ω–∏–µ
source .venv/bin/activate

# –ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ —Ç–µ—Å—Ç—ã
pytest

# –¢–µ—Å—Ç—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –º–æ–¥—É–ª—è
pytest tests/test_llm_clients.py -v

# –° –ø–æ–∫—Ä—ã—Ç–∏–µ–º
pytest --cov=tg_parser --cov-report=html

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–∏–Ω—Ç–∏–Ω–≥
ruff check .
ruff format .

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å Docker
docker build -t tg_parser:test .
docker run tg_parser:test --help
```

---

## ‚úÖ –ö—Ä–∏—Ç–µ—Ä–∏–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ v1.2.0

### Must Have
- [ ] ‚≠ê AnthropicClient —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] ‚≠ê OllamaClient —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] Factory —Å–æ–∑–¥–∞—ë—Ç –∫–ª–∏–µ–Ω—Ç—ã –ø–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—É
- [ ] `--provider` –∏ `--model` –≤ CLI
- [ ] –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (`--concurrency`)
- [ ] PromptLoader –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω –≤ pipeline

### Should Have
- [ ] GeminiClient —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] Dockerfile —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] GitHub Actions CI
- [ ] Progress bars –≤ CLI
- [ ] –û–±—Ä–∞–±–æ—Ç–∫–∞ 846 —Å–æ–æ–±—â–µ–Ω–∏–π < 10 –º–∏–Ω

### Nice to Have
- [ ] docker-compose.yml
- [ ] Dry-run mode
- [ ] LLM response caching
- [ ] CONTRIBUTING.md

---

## üìä Success Metrics

| –ú–µ—Ç—Ä–∏–∫–∞ | –¢–µ–∫—É—â–µ–µ | –¶–µ–ª—å |
|---------|---------|------|
| **LLM providers** | 1 (OpenAI) | 4 (+ Anthropic, Gemini, Ollama) |
| **Processing time** | 30 –º–∏–Ω / 846 msgs | < 10 –º–∏–Ω |
| **Test count** | 103 | 120+ |
| **Docker support** | ‚ùå | ‚úÖ |
| **CI/CD** | ‚ùå | ‚úÖ |

---

## üöÄ –ö–æ–º–∞–Ω–¥—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞

```bash
# 1. –ü—Ä–æ—á–∏—Ç–∞—Ç—å handoff v1.1
cat docs/notes/SESSION_HANDOFF_v1.1.md

# 2. –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Ç–µ–∫—É—â–∏–π OpenAI client (–æ–±—Ä–∞–∑–µ—Ü)
cat tg_parser/processing/llm/openai_client.py

# 3. –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å LLMClient
cat tg_parser/processing/ports.py

# 4. –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã
pytest --tb=short -q

# 5. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å settings
cat tg_parser/config/settings.py
```

---

## ‚ö†Ô∏è –í–∞–∂–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

1. **–ù–µ –ª–æ–º–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ—Å—Ç—ã** ‚Äî –≤—Å–µ 103 –¥–æ–ª–∂–Ω—ã –ø—Ä–æ—Ö–æ–¥–∏—Ç—å
2. **Backward compatibility** ‚Äî OpenAI –¥–æ–ª–∂–µ–Ω —Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∫ —Ä–∞–Ω—å—à–µ
3. **–ï–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å** ‚Äî –≤—Å–µ LLM –∫–ª–∏–µ–Ω—Ç—ã —Ä–µ–∞–ª–∏–∑—É—é—Ç `LLMClient`
4. **Rate limiting** ‚Äî —É—á–∏—Ç—ã–≤–∞—Ç—å –ª–∏–º–∏—Ç—ã –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
5. **API keys –≤ ENV** ‚Äî –Ω–µ —Ö–∞—Ä–¥–∫–æ–¥–∏—Ç—å –≤ –∫–æ–¥–µ

---

## üîó –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

- [DEVELOPMENT_ROADMAP.md](../../DEVELOPMENT_ROADMAP.md) ‚Äî –ø–æ–ª–Ω—ã–π roadmap
- [docs/notes/SESSION_HANDOFF_v1.1.md](SESSION_HANDOFF_v1.1.md) ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã v1.1
- [CHANGELOG.md](../../CHANGELOG.md) ‚Äî –∏—Å—Ç–æ—Ä–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
- [prompts/README.md](../../prompts/README.md) ‚Äî —Ñ–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç–æ–≤

---

## üí¨ –ö–æ–Ω—Ç–∞–∫—Ç –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∞–≥–µ–Ω—Ç–∞

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å–æ–∑–¥–∞–π —Ñ–∞–π–ª `docs/notes/SESSION_HANDOFF_v1.2.md` —Å:
1. –°–ø–∏—Å–∫–æ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
2. –°–ø–∏—Å–∫–æ–º –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
3. –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ —Ä–∞–∑–Ω—ã—Ö LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
4. –ò–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –ø—Ä–æ–±–ª–µ–º–∞–º–∏
5. –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –¥–ª—è v2.0 –∞–≥–µ–Ω—Ç–∞ (GPT-5 / Agents SDK)

---

**Version**: 1.0  
**Created**: 26 –¥–µ–∫–∞–±—Ä—è 2025  
**Target**: v1.2.0 release  
**ETA**: 4 –Ω–µ–¥–µ–ª–∏  
**Previous session**: Session 11 (v1.1 Configurable Prompts)  
**Next session**: Session 13 (v2.0 GPT-5 / Agents SDK)

---

**–ì–æ—Ç–æ–≤ –∫ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏! –ù–∞—á–Ω–∏ —Å –ó–∞–¥–∞—á–∏ 1 ‚Äî Multi-LLM Support.** üöÄ

