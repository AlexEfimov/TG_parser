# Session 15 ‚Äî Phase 2E: Hybrid Agent Mode

**–î–∞—Ç–∞**: 27 –¥–µ–∫–∞–±—Ä—è 2025  
**Branch**: `feature/v2.0-agents`  
**–ü—Ä–µ–¥—ã–¥—É—â–∏–π —ç—Ç–∞–ø**: Phase 2C/2D (Session 14)  
**–§–æ–∫—É—Å**: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è v1.2 pipeline –∫–∞–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –∞–≥–µ–Ω—Ç–∞

---

## üìã –¶–µ–ª—å Phase 2E

–°–æ–∑–¥–∞—Ç—å **–≥–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º** –∞–≥–µ–Ω—Ç–∞, –≥–¥–µ:
1. Agent –º–æ–∂–µ—Ç –≤—ã–∑—ã–≤–∞—Ç—å **v1.2 pipeline** –∫–∞–∫ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
2. Agent —Å–∞–º —Ä–µ—à–∞–µ—Ç, –∫–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å pipeline vs —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ tools
3. –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –æ–±–æ–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤

---

## üîß –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ

### –ß—Ç–æ —É–∂–µ –µ—Å—Ç—å:

#### v1.2 Pipeline (`tg_parser/processing/pipeline.py`):
```python
class ProcessingPipelineImpl(ProcessingPipeline):
    """
    LLM-based processing —Å:
    - –î–µ—Ç–∞–ª—å–Ω—ã–º parsing JSON –æ—Ç–≤–µ—Ç–æ–≤
    - Retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º (3 –ø–æ–ø—ã—Ç–∫–∏, backoff)
    - –ó–∞–ø–∏—Å—å—é –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
    - –ü–æ–¥–¥–µ—Ä–∂–∫–æ–π –≤—Å–µ—Ö LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
    """
    
    async def process_message(
        self, 
        message: RawTelegramMessage, 
        force: bool = False
    ) -> ProcessedDocument
```

#### Agent Tools (`tg_parser/agents/tools/text_tools.py`):
```python
# Basic tools (pattern matching, –±—ã—Å—Ç—Ä—ã–µ)
clean_text(text) -> CleanTextResult
extract_topics(text, max_topics) -> TopicsResult  
extract_entities(text) -> EntitiesResult

# LLM-enhanced tools (–≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑)
analyze_text_deep(ctx, text) -> DeepAnalysisResult
extract_topics_llm(ctx, text) -> TopicsResult
extract_entities_llm(ctx, text) -> EntitiesResult
```

#### Agent Context:
```python
@dataclass
class AgentContext:
    llm_client: Any = None
    use_llm_tools: bool = True
    provider: str = "openai"
    model: str = "gpt-4o-mini"
    extra: dict = field(default_factory=dict)
```

---

## üéØ –¶–µ–ª–µ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Phase 2E

### –ù–æ–≤—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç ‚Äî `process_with_pipeline`:

```python
# tg_parser/agents/tools/pipeline_tool.py

from agents import RunContextWrapper, function_tool
from pydantic import BaseModel, Field

class PipelineResult(BaseModel):
    """Result from v1.2 pipeline processing."""
    text_clean: str
    summary: str | None
    topics: list[str]
    entities: list[dict]
    language: str
    metadata: dict = Field(default_factory=dict)


@function_tool
async def process_with_pipeline(
    ctx: RunContextWrapper[AgentContext],
    text: Annotated[str, "Raw text to process with v1.2 pipeline"],
    channel_id: Annotated[str, "Channel identifier"] = "unknown",
    message_id: Annotated[int, "Message ID"] = 0,
) -> PipelineResult:
    """
    Process text using the proven v1.2 LLM pipeline.
    
    Use this tool when:
    - Text requires deep semantic analysis
    - You need reliable entity/topic extraction
    - Basic tools are insufficient
    
    This tool uses the full LLM pipeline with:
    - Configurable prompts (YAML-based)
    - Retry mechanism
    - Multi-LLM support
    """
    # –ü–æ–ª—É—á–∏—Ç—å pipeline –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    pipeline = ctx.context.extra.get("pipeline")
    if not pipeline:
        # Fallback: —Å–æ–∑–¥–∞—Ç—å pipeline –Ω–∞ –ª–µ—Ç—É
        pipeline = await _create_pipeline(ctx.context)
    
    # –°–æ–∑–¥–∞—Ç—å RawTelegramMessage
    message = RawTelegramMessage(
        id=str(uuid4()),
        source_ref=f"agent_request_{message_id}",
        channel_id=channel_id,
        text=text,
        date=datetime.now(UTC),
        raw_json={},
    )
    
    # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —á–µ—Ä–µ–∑ pipeline
    doc = await pipeline.process_message(message, force=True)
    
    return PipelineResult(
        text_clean=doc.text_clean,
        summary=doc.summary,
        topics=doc.topics,
        entities=[e.model_dump() for e in doc.entities],
        language=doc.language,
        metadata=doc.metadata,
    )
```

### –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π AgentContext:

```python
@dataclass
class AgentContext:
    llm_client: Any = None
    use_llm_tools: bool = True
    provider: str = "openai"
    model: str = "gpt-4o-mini"
    # NEW: v1.2 pipeline instance
    pipeline: ProcessingPipelineImpl | None = None
    extra: dict = field(default_factory=dict)
```

### –ù–æ–≤—ã–π —Ä–µ–∂–∏–º –∞–≥–µ–Ω—Ç–∞ ‚Äî Hybrid:

```python
class TGProcessingAgent:
    def __init__(
        self,
        model: str = "gpt-4o-mini",
        provider: str = "openai",
        use_llm_tools: bool = False,
        use_pipeline_tool: bool = False,  # NEW
        llm_client: Any = None,
        pipeline: ProcessingPipelineImpl | None = None,  # NEW
    ):
        ...
    
    @property
    def agent(self) -> Agent:
        if self._agent is None:
            # Choose tools based on configuration
            tools = [clean_text, extract_topics, extract_entities]
            
            if self.use_llm_tools:
                tools.append(analyze_text_deep)
            
            if self.use_pipeline_tool:
                tools.append(process_with_pipeline)  # NEW
            
            ...
```

---

## üìù CLI –∏–∑–º–µ–Ω–µ–Ω–∏—è

### –ù–æ–≤—ã–µ —Ñ–ª–∞–≥–∏:

```bash
# Hybrid mode: agent –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å v1.2 pipeline
tg-parser process --channel @lab --agent --hybrid

# –ü–æ–ª–Ω—ã–π —Ä–µ–∂–∏–º: agent + LLM tools + pipeline tool
tg-parser process --channel @lab --agent --agent-llm --hybrid
```

### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤ `process_cmd.py`:

```python
@app.command()
def process(
    channel: str,
    agent: bool = typer.Option(False, "--agent", help="Use agent-based processing"),
    agent_llm: bool = typer.Option(False, "--agent-llm", help="Enable LLM tools in agent"),
    hybrid: bool = typer.Option(False, "--hybrid", help="Enable v1.2 pipeline as tool"),  # NEW
    ...
):
    if agent:
        use_pipeline_tool = hybrid
        # Create TGProcessingAgent with pipeline tool
```

---

## üß™ –¢–µ—Å—Ç—ã

### –ù–æ–≤—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è Phase 2E:

```python
# tests/test_agents_phase2e.py

class TestPipelineTool:
    """Tests for process_with_pipeline tool."""
    
    def test_pipeline_tool_exists(self):
        from tg_parser.agents.tools.pipeline_tool import process_with_pipeline
        assert callable(process_with_pipeline)
    
    async def test_pipeline_tool_with_mock_pipeline(self):
        ...
    
    async def test_pipeline_tool_fallback(self):
        """Test fallback when pipeline not in context."""
        ...


class TestHybridAgent:
    """Tests for hybrid agent mode."""
    
    def test_agent_with_pipeline_tool(self):
        agent = TGProcessingAgent(
            use_pipeline_tool=True,
        )
        assert len(agent.agent.tools) == 4  # 3 basic + pipeline
    
    def test_agent_full_hybrid(self):
        agent = TGProcessingAgent(
            use_llm_tools=True,
            use_pipeline_tool=True,
        )
        # analyze_text_deep + process_with_pipeline
        ...


class TestCLIHybridFlag:
    """Tests for --hybrid CLI flag."""
    
    def test_hybrid_flag_parsing(self):
        ...
```

---

## üìä –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∂–∏–º–æ–≤:

| –†–µ–∂–∏–º | –°–∫–æ—Ä–æ—Å—Ç—å | –ö–∞—á–µ—Å—Ç–≤–æ | LLM Calls | Use Case |
|-------|----------|----------|-----------|----------|
| Basic Agent | ‚ö° Fast | ‚≠ê‚≠ê | 1 (orchestration) | Quick triage |
| LLM Agent | üê¢ Slow | ‚≠ê‚≠ê‚≠ê‚≠ê | 2+ | Deep analysis |
| Hybrid Agent | üîÑ Adaptive | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 1-3 | Best of both |
| Pipeline v1.2 | üê¢ Slow | ‚≠ê‚≠ê‚≠ê‚≠ê | 1 | Proven results |

### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Hybrid Mode:

1. **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å**: Agent —Å–∞–º —Ä–µ—à–∞–µ—Ç, –∫–æ–≥–¥–∞ –Ω—É–∂–µ–Ω pipeline
2. **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å**: –ü—Ä–æ—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è ‚Üí basic tools, —Å–ª–æ–∂–Ω—ã–µ ‚Üí pipeline
3. **–ù–∞–¥—ë–∂–Ω–æ—Å—Ç—å**: Fallback –Ω–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π pipeline
4. **Flexibility**: –ú–æ–∂–Ω–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å tools –ø–æ —Å–∏—Ç—É–∞—Ü–∏–∏

---

## üìÅ –§–∞–π–ª—ã –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è

```
tg_parser/agents/
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # Export pipeline_tool
‚îÇ   ‚îú‚îÄ‚îÄ text_tools.py            # Update AgentContext
‚îÇ   ‚îî‚îÄ‚îÄ pipeline_tool.py         # NEW: process_with_pipeline
‚îú‚îÄ‚îÄ processing_agent.py          # Add use_pipeline_tool, hybrid support
‚îî‚îÄ‚îÄ __init__.py                  # Export new components

tg_parser/cli/
‚îú‚îÄ‚îÄ app.py                       # Add --hybrid flag
‚îî‚îÄ‚îÄ process_cmd.py               # Handle hybrid mode

tests/
‚îú‚îÄ‚îÄ test_agents.py               # Update existing tests
‚îî‚îÄ‚îÄ test_agents_phase2e.py       # NEW: Phase 2E tests
```

---

## ‚úÖ –ö—Ä–∏—Ç–µ—Ä–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è Phase 2E

1. [ ] `process_with_pipeline` tool —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω
2. [ ] `AgentContext` —Ä–∞—Å—à–∏—Ä–µ–Ω –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è pipeline
3. [ ] `TGProcessingAgent` –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç `use_pipeline_tool`
4. [ ] CLI —Ñ–ª–∞–≥ `--hybrid` —Ä–∞–±–æ—Ç–∞–µ—Ç
5. [ ] 10+ –Ω–æ–≤—ã—Ö —Ç–µ—Å—Ç–æ–≤
6. [ ] –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞
7. [ ] –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π benchmark (basic vs llm vs hybrid vs pipeline)

---

## üöÄ –°–ª–µ–¥—É—é—â–∏–µ —ç—Ç–∞–ø—ã

–ü–æ—Å–ª–µ Phase 2E:

| –≠—Ç–∞–ø | –ù–∞–∑–≤–∞–Ω–∏–µ | –û–ø–∏—Å–∞–Ω–∏–µ |
|------|----------|----------|
| **Phase 2F** | API Production | Auth, rate limiting, webhooks –¥–ª—è HTTP API |
| **Phase 3A** | Multi-Agent | –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã (cleaner, extractor, summarizer) |
| **Phase 3B** | RAG Integration | –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫, embeddings, knowledge base |
| **Phase 3C** | Web UI | Dashboard –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–æ–π |

---

## üìé –°—Å—ã–ª–∫–∏

- **–ü—Ä–µ–¥—ã–¥—É—â–∏–π —ç—Ç–∞–ø**: `docs/notes/SESSION14_PHASE2C_COMPLETE.md`
- **v1.2 Pipeline**: `tg_parser/processing/pipeline.py`
- **Agent Tools**: `tg_parser/agents/tools/text_tools.py`
- **Processing Agent**: `tg_parser/agents/processing_agent.py`
- **OpenAI Agents SDK**: https://github.com/openai/openai-agents-python

---

**–ì–æ—Ç–æ–≤ –∫ –Ω–∞—á–∞–ª—É Phase 2E!**

