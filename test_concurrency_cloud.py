#!/usr/bin/env python3
"""
Cloud Providers Concurrency Test –¥–ª—è v1.2.0
–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ concurrency –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–±–ª–∞—á–Ω—ã—Ö LLM
(OpenAI, Anthropic, Gemini)
"""
import asyncio
import time
import json
from pathlib import Path

from tg_parser.config import settings
from tg_parser.processing import create_processing_pipeline
from tg_parser.storage.sqlite import Database, DatabaseConfig
from tg_parser.storage.sqlite.processed_document_repo import SQLiteProcessedDocumentRepo
from tg_parser.storage.sqlite.processing_failure_repo import SQLiteProcessingFailureRepo
from tg_parser.storage.sqlite.raw_message_repo import SQLiteRawMessageRepo


async def test_provider_concurrency(
    provider: str,
    model: str,
    concurrency_levels: list[int] = [1, 3, 5],
    message_count: int = 15,
    channel_id: str = "labdiagnostica_logical"
):
    """–¢–µ—Å—Ç –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ concurrency"""
    print(f"\n{'='*80}")
    print(f"üß™ Testing {provider.upper()} - {model}")
    print(f"   Concurrency levels: {concurrency_levels}")
    print(f"{'='*80}\n")
    
    provider_results = []
    baseline_time = None
    
    for concurrency in concurrency_levels:
        print(f"\n‚ö° Testing concurrency={concurrency}")
        print("-" * 80)
        
        # Database setup
        config = DatabaseConfig(
            ingestion_state_path=settings.ingestion_state_db_path,
            raw_storage_path=settings.raw_storage_db_path,
            processing_storage_path=settings.processing_storage_db_path,
        )
        db = Database(config)
        await db.init()
        
        try:
            raw_session = db.raw_storage_session()
            processing_session = db.processing_storage_session()
            
            try:
                # –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏
                raw_repo = SQLiteRawMessageRepo(raw_session)
                processed_repo = SQLiteProcessedDocumentRepo(processing_session)
                failure_repo = SQLiteProcessingFailureRepo(processing_session)
                
                # Pipeline
                pipeline = create_processing_pipeline(
                    provider=provider,
                    model=model,
                    processed_doc_repo=processed_repo,
                    failure_repo=failure_repo,
                )
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º raw —Å–æ–æ–±—â–µ–Ω–∏—è
                all_raw = await raw_repo.list_by_channel(channel_id)
                if not all_raw:
                    print(f"‚ùå No raw messages found")
                    continue
                
                raw_messages = all_raw[:message_count]
                print(f"üì¶ Processing {len(raw_messages)} messages")
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞
                start = time.time()
                processed_docs = await pipeline.process_batch(
                    raw_messages,
                    force=True,
                    concurrency=concurrency
                )
                elapsed = time.time() - start
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                success_count = len(processed_docs)
                failed_count = len(raw_messages) - success_count
                throughput = len(raw_messages) / elapsed if elapsed > 0 else 0
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º baseline
                if concurrency == concurrency_levels[0]:
                    baseline_time = elapsed
                
                speedup = baseline_time / elapsed if baseline_time and elapsed > 0 else 1.0
                
                print(f"  ‚úÖ Success: {success_count}/{len(raw_messages)} ({success_count/len(raw_messages)*100:.1f}%)")
                print(f"  ‚è±Ô∏è  Total time: {elapsed:.2f}s")
                print(f"  ‚ö° Throughput: {throughput:.3f} msg/sec")
                print(f"  üìà Speedup: {speedup:.2f}x")
                
                provider_results.append({
                    "concurrency": concurrency,
                    "success_count": success_count,
                    "failed_count": failed_count,
                    "total_time": elapsed,
                    "throughput": throughput,
                    "speedup": speedup,
                })
                
            finally:
                await raw_session.close()
                await processing_session.close()
                if hasattr(pipeline, "llm_client") and hasattr(pipeline.llm_client, "close"):
                    await pipeline.llm_client.close()
        finally:
            await db.close()
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ç–µ—Å—Ç–∞–º–∏
        if concurrency != concurrency_levels[-1]:
            print(f"\n‚è∏Ô∏è  Waiting 3 seconds before next concurrency level...")
            await asyncio.sleep(3)
    
    return {
        "provider": provider,
        "model": model,
        "results": provider_results
    }


async def main():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å concurrency —Ç–µ—Å—Ç—ã –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    print("üöÄ TG_parser v1.2.0 ‚Äî Cloud Providers Concurrency Testing")
    print("=" * 80)
    print("\nüìã Test Configuration:")
    print("  ‚Ä¢ Message count: 15 per test")
    print("  ‚Ä¢ Concurrency levels: [1, 3, 5]")
    print("  ‚Ä¢ Force reprocess: Yes")
    print("  ‚Ä¢ Providers: OpenAI, Anthropic, Gemini")
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤
    providers = [
        ("openai", "gpt-4o-mini"),
        ("anthropic", "claude-3-5-sonnet-20241022"),
        ("gemini", "gemini-2.0-flash-exp"),
    ]
    
    all_results = []
    
    for provider, model in providers:
        try:
            result = await test_provider_concurrency(
                provider, 
                model,
                concurrency_levels=[1, 3, 5],
                message_count=15
            )
            all_results.append(result)
        except Exception as e:
            print(f"\n‚ùå Test failed for {provider}: {e}")
            import traceback
            traceback.print_exc()
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏
        if provider != providers[-1][0]:
            print(f"\n‚è∏Ô∏è  Waiting 10 seconds before next provider...")
            await asyncio.sleep(10)
    
    # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    print(f"\n{'='*80}")
    print("üìä CONCURRENCY COMPARISON")
    print(f"{'='*80}\n")
    
    for provider_data in all_results:
        print(f"\nüîπ {provider_data['provider'].upper()} ({provider_data['model']})")
        print(f"{'Concurrency':<15} {'Time':<12} {'Throughput':<18} {'Speedup':<12}")
        print("-" * 80)
        
        for r in provider_data['results']:
            print(
                f"{r['concurrency']:<15} "
                f"{r['total_time']:.2f}s{' '*6} "
                f"{r['throughput']:.3f} msg/s{' '*6} "
                f"{r['speedup']:.2f}x"
            )
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print(f"\nüí° Optimal Concurrency Recommendations:")
    for provider_data in all_results:
        if provider_data['results']:
            best = max(provider_data['results'], key=lambda x: x['throughput'])
            print(f"  ‚Ä¢ {provider_data['provider']}: concurrency={best['concurrency']} (throughput: {best['throughput']:.3f} msg/s)")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_path = Path("test_results_concurrency.json")
    with open(output_path, "w") as f:
        json.dump(all_results, f, indent=2, default=str)
    print(f"\nüíæ Results saved to: {output_path}")


if __name__ == "__main__":
    asyncio.run(main())

